# EXP-02: Voice-Telemetry Correlation

> **Status**: Active  
> **Initiated**: 2026-01-22  
> **Researcher**: Little Padawan + Master Lonn  
> **Domain**: Multi-modal data fusion for motor skill analysis

---

## 1. Research Question

**Can real-time voice commentary synchronized with telemetry data reveal cognitive-motor correlations that are invisible in telemetry data alone?**

Sub-questions:
1. Does verbal self-cueing correlate with actual technique execution?
2. Can we track mental state progression through voice analysis?
3. What is the relationship between "what the driver thinks happened" and "what actually happened"?
4. Does voice data improve coaching feedback specificity?

---

## 2. Hypothesis

Real-time voice commentary synchronized with telemetry data enables:

1. **Technique Validation**: Verbal cues ("no trail braking") can be correlated with actual inputs to confirm technique deployment
2. **Mental State Tracking**: Emotional progression (frustration ‚Üí acceptance ‚Üí focus) is captured in voice but invisible in telemetry
3. **Perception-Reality Gap**: Comparison between driver's stated experience and objective data reveals calibration accuracy
4. **Richer Coaching Data**: Voice context explains telemetry anomalies that would otherwise be ambiguous

---

## 3. Methodology

### 3.1 Data Collection

**Hardware:**
- iRacing telemetry (IBT file) ‚Äî car inputs, physics, timing
- Voice recording app (Dote or similar) ‚Äî continuous audio during session
- Optional: CrewChief (Jim) audio for additional sync anchors

**Recording Protocol:**
1. Start voice recording before entering car
2. Verbalize key moments: "green flag", corner callouts, technique reminders, emotional states
3. Stop recording after session ends
4. Export voice to JSON transcript format (Dote exports timestamps + text)

### 3.2 Synchronization Method

**Anchor Point System:**
- Primary anchor: Race start ("green green green" or similar distinctive callout)
- Secondary anchors: Lap completions, CrewChief callouts, corner names
- Calculate offset: `voice_timestamp - anchor_timestamp = session_offset`
- Apply offset to all voice entries for IBT correlation

**Sync Formula:**
```
IBT_time = voice_timestamp - sync_anchor_voice_time
Track_position = IBT_LapDistPct at IBT_time
```

### 3.3 Analysis Framework

| Data Source | What It Captures | Analysis Method |
|-------------|------------------|-----------------|
| IBT telemetry | Car behavior (inputs, physics) | Existing tools (extract_session, analyze_technique) |
| Voice transcript | Cognitive state, intentions | Timestamp correlation, keyword extraction |
| Merged dataset | Cognitive-motor correlation | Compare stated intent vs actual execution |

### 3.4 Key Metrics

**Technique Deployment:**
- Verbal cue frequency (e.g., "no trail braking" count)
- Cue-to-execution correlation (did saying it = doing it?)
- Technique consistency when verbalized vs not

**Mental State:**
- Emotional keyword tracking (frustration, focus, confidence)
- Recovery speed (time from negative to neutral/positive)
- Peak performance timing relative to mental state

**Perception Accuracy:**
- "Good lap" statements vs actual lap times
- "Mistake" callouts vs telemetry anomalies
- Self-assessment calibration score

---

## 4. Session Log

### Session 01: Summit Point AI Race (2026-01-22)

**Context:**
- First voice-telemetry session
- AI Race (12 min, 11 opponents)
- High-pressure conditions: P1 start, contact, off-track, P8‚ÜíP2 recovery

**Files:**
- IBT: `data/processed/2026-01-22-09-10-summit-ai-race-voice.ibt`
- Transcript: `data/processed/2026-01-22-09-10-summit-ai-race-voice-transcript.json`
- Report: `weeks/week07/2026-01-22-09-10-summit-ai-race-voice.md`

**Sync Anchor:**
- Voice: "green, green, green" at `00:11:24.640`
- IBT: Race start = T=0
- Offset: 684.64 seconds (11:24.640)

**Key Findings:**

#### Finding 1: Verbal Self-Cueing Correlates with Execution

Master Lonn said "no trail braking" at T5 (Carousel) **5+ times** throughout the race.

| Voice Timestamp | Voice Content | IBT Validation |
|-----------------|---------------|----------------|
| 12:11:300 | "not trailbraking" | Carousel Entry executed with coasting |
| 17:34:720 | "no trail braking" | Technique maintained in traffic |
| 18:54:540 | "not trail braking" | Technique maintained under pressure |
| 22:53:180 | "no trail braking" | Final lap technique deployment |

**Data Proof:**
- Carousel oversteer: 2,675 events (vs 5,717 yesterday = **-53%**)
- Carousel Entry brake œÉ: **0.8 meters** (automatic consistency)
- Technique verbalized AND executed consistently

**Conclusion:** Saying the technique = doing the technique. Verbal self-cueing works.

#### Finding 2: Mental State Progression Trackable

| Time | Voice Content | Emotional State | IBT Context |
|------|---------------|-----------------|-------------|
| 11:47 | "kept everybody behind me" | Confident | Lap 1 clean, P1 |
| 13:12 | "got contact... nasty" | Frustrated | Lap 2 contact |
| 13:27 | "fifth, sixth, seventh" | Accepting reality | Dropping positions |
| 13:42 | "Let's try to settle in" | Resetting | P8, recovery mode |
| 15:27 | "Yes, I will take the first one" | Hunting | Fighting back |
| 22:26 | "Getting in control of my nerves" | Composed | White flag, P3 |
| 23:22 | "Better exit? Yes I have" | Confident | Final pass for P2 |

**Recovery Timeline:**
- Incident: 14:32 ("off track, lost control")
- Reset initiated: 14:45 ("didn't went as planned")
- Race pace recovered: Lap 4 (1:18.283) = **~90 seconds** to full recovery

**Conclusion:** Voice captures emotional arc invisible in telemetry.

#### Finding 3: Peak Performance After Pressure

| Metric | Value | Context |
|--------|-------|---------|
| Fastest lap | 1:17.250 | Lap 9 (FINAL lap) |
| Worst lap | 1:27.767 | Lap 3 (incident) |
| Time between | 6 laps | ~8 minutes |

Voice at fastest lap: "Better exit? Yes I have. And I overtake the last second car as well."

**Conclusion:** Peak performance came AFTER pressure resolved, not before. Mental composure enables speed.

#### Finding 4: Voice Explains Telemetry Anomalies

**Example 1:** Lap 7 slower (1:20.717) than surrounding laps
- Telemetry alone: Unknown cause
- Voice context: "they're really slow... I have to make sure I'm not driving into the back of them"
- Explanation: Stuck behind slower AI, deliberate pace management

**Example 2:** Lap 3 catastrophic (1:27.767)
- Telemetry alone: T1 = 12.85s (vs normal 5.2s)
- Voice context: "Ohh, sliding... Getting in contact now... off track, lost control"
- Explanation: Contact-induced spin, not driver error

**Conclusion:** Voice provides WHY behind telemetry WHAT.

---

## 5. Methodology Validation

### What Worked ‚úÖ

1. **Single sync anchor sufficient**: "green green green" provided reliable T=0
2. **Natural verbalization**: Driver spoke naturally without disrupting flow
3. **Keyword extraction viable**: Technique cues ("no trail braking") easily searchable
4. **Emotional progression clear**: State changes visible in transcript
5. **Dote JSON format usable**: Timestamps + text structure works for correlation

### What Needs Improvement üîß

1. **Manual correlation**: Currently comparing by hand, need automated tool
2. **Approximate sync**: Voice timestamps are speech boundaries, not exact moments
3. **No CrewChief integration**: Adding Jim's callouts would provide more anchors
4. **Single session**: Need more data points for pattern validation

### Tool Requirements

**Proposed: `tools/coach/merge_voice_telemetry.py`**

Inputs:
- IBT file path
- Transcript JSON path
- Sync anchor voice timestamp
- Sync anchor type (race_start, lap_complete, etc.)

Outputs:
- Merged timeline JSON with voice annotations at track positions
- Keyword frequency analysis
- Mental state progression chart
- Technique cue correlation report

---

## 6. Research Implications

### For ADHD-Adapted Learning

Voice recording may serve as:
1. **Focus anchor**: Verbalizing technique keeps ADHD brain on task
2. **Memory aid**: Audio record supplements working memory limitations
3. **Self-awareness tool**: Hearing yourself reveals patterns invisible in the moment
4. **Engagement mechanism**: "Talking to Little Wan" maintains motivation

### For Human-AI Collaboration

Voice data enables AI coach to:
1. **Understand context**: Why anomalies occurred, not just that they occurred
2. **Track mental state**: Adapt coaching tone to emotional state
3. **Validate technique deployment**: Confirm advice was actually followed
4. **Personalize feedback**: Reference driver's own words back to them

### For Motor Skill Research

Multi-modal data fusion provides:
1. **Cognitive-motor correlation**: Link intention to action
2. **Learning progression tracking**: How self-talk evolves with skill
3. **Pressure response analysis**: Mental state under competitive conditions
4. **Perception calibration**: Accuracy of driver self-assessment

---

## 7. Next Steps

### Immediate (Next Session)

- [ ] Add CrewChief (Jim) audio recording for additional sync points
- [ ] Test in official race for higher pressure conditions
- [ ] Capture explicit "this felt good/bad" callouts for perception calibration

### Short-term (This Week)

- [ ] Build `tools/coach/merge_voice_telemetry.py` prototype
- [ ] Define keyword taxonomy (technique cues, emotional states, assessments)
- [ ] Create visualization for voice-annotated lap evolution

### Medium-term (Next Month)

- [ ] Collect 5+ voice sessions for pattern analysis
- [ ] Compare voice patterns between good and bad sessions
- [ ] Develop "verbal self-cueing protocol" for technique deployment
- [ ] Document methodology for guidebook (Chapter 13 extension?)

---

## 8. References

### Internal

- Session Report: `weeks/week07/2026-01-22-09-10-summit-ai-race-voice.md`
- learning_memory.json: EXP-02 entry
- Related: EXP-01 (Focused Practice Threshold)

### Tools Used

- `tools/core/extract_session_from_ibt.py`
- `tools/coach/analyze_ibt_technique.py`
- `tools/coach/detect_brake_point_drift.py`
- Voice recording: Dote app (iOS)

### Related Research Areas

- Verbal self-instruction in motor learning
- Think-aloud protocols in expertise research
- Multi-modal learning analytics
- Affective computing in sports performance

---

## 9. Appendix: Raw Transcript Excerpts

### Race Start Sequence
```
11:22:740 | "Grid up on the right side"
11:24:640 | "And there is the red light"
11:25:680 | "And green, green, green"
11:33:160 | "good start here"
11:47:720 | "yeah I kept everybody behind me in the first turn"
```

### Incident Sequence
```
14:14:840 | "Going towards turn 1"
14:21:420 | "Ohh, sliding"
14:23:860 | "Getting in contact now"
14:32:540 | "And off track, lost control"
14:37:379 | "And now I'm lost"
14:45:100 | "well that didn't went as planned"
```

### Recovery & Climb
```
14:52:920 | "see if I can follow these guys and take over someone"
15:27:840 | "Yes, I will take the first one"
17:23:160 | "just passed another car"
22:17:160 | "Third position, white flag"
22:26:060 | "Getting in control of my nerves now"
23:27:500 | "And I overtake the last second car as well"
23:34:260 | "So I will finish right about now in second place"
```

### T5 Technique Deployment (All Instances)
```
12:11:300 | "Not trailbraking"
17:34:720 | "no trail braking"
18:54:540 | "into one not trail braking"
20:14:180 | "Not railbraking"
22:53:180 | "Back in 2-1, no trail braking"
```

---

*"The voice captures what the car cannot tell us."* üéôÔ∏èüèéÔ∏è

---

**Document Version:** 1.0  
**Last Updated:** 2026-01-22  
**Author:** Little Padawan (with Master Lonn)
